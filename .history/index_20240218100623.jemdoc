# jemdoc: menu{MENU}{index.html}, nofooter
= Yajie Bao

~~~
{}{img_left}{figs/gulangyu.jpeg}{Me}{300}{225}
PhD candidate, Statistics\n
[http://www.math.sjtu.edu.cn/ School of Mathematical Sciences]\n 
[http://www.math.sjtu.edu.cn/ Shanghai Jiao Tong University]\n
Advisor: [http://www.math.sjtu.edu.cn/faculty/show.php?id=35 Dong Han]\n
Email: baoyajie2019stat@sjtu.edu.cn\n
[https://scholar.google.com/citations?user=1n_aUsIAAAAJ&hl=en Google scholar]
~~~

== About Me
I am currently a final-year Ph.D. student in Statistics at [http://www.math.sjtu.edu.cn/ Shanghai Jiao Tong University]. Before coming to SJTU, I received my B.S. degree in Statistics from [http://stat.ecnu.edu.cn East China Normal University].
I have been working with [https://mingrliu.github.io/index.html Mingrui Liu] on federated learning and deep learning theory since November 2021. At SJTU, I'm a member of [https://sites.google.com/view/haojieren Haojie Ren]'s research group on conformal inference. 
I was fortunate to visit Prof. [http://web.stat.nankai.edu.cn/chlzou/ Changliang Zou]'s group at Nankai University from April-August, 2023.

== Research interests
- Comformal inference
- Deep learning theory
- Federated learning

== Recent papers
- [https://arxiv.org/pdf/2301.00584.pdf Selective conformal inference with  false coverage-statement rate control]\n
Yajie Bao, Yuyang Huo, Haojie Ren and Changliang Zou.\n
/Biometrika/, 2024, accepted.

- [https://openreview.net/pdf?id=gVLKXT9JwG Global Convergence Analysis of Local SGD for Two-layer Neural Network without Overparameterization]\n
Yajie Bao, Amarda Shehu, Mingrui Liu.\n
/Advances in Neural Information Processing Systems (NeurIPS)/, 2023.

-  [https://openreview.net/pdf?id=Yq6GKgN3RC Federated Learning with Client Subsampling, Data Heterogeneity, and Unbounded Smoothness: A New Algorithm and Lower Bounds]\n
Michael Crawshaw, Yajie Bao and Mingrui Liu.\n
/Advances in Neural Information Processing Systems (NeurIPS)/, 2023.


== Service
Reviewer: ICML 2022; NeurIPS 2022, 2023; ICLR 2024; AISTATS 2024.
