---
title: "Bayes Estimator and Minimax Estimation"

output:
  html_document:
    css: style.css
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

<br />

> This blog is essentially based on lecture 8 of [CMU STAT 705](http://www.stat.cmu.edu/~larry/=stat705/Lecture8.pdf) taught by Larry Wasserman and the book [Theory of Point Estimation](https://www.springer.com/gp/book/9780387985022) by E.L. Lehmann and George Casella.


# Bayes Estimator

Suppose we need to estimate the parameter $\theta$ based on sample $\{X_1,...,X_n\}$. And we use loss function $L(\theta,\hat\theta(X))$ to measure the performance of estimator $\hat\theta(X)$. The **risk** of estimator $\hat\theta(X)$ is
$$
R(\theta,\hat\theta(X))=\mathbb{E}_{\theta}\left[L(\theta,\hat\theta(X))\right]=\int L\left(\theta, \widehat{\theta}\left(x_{1}, \ldots, x_{n}\right)\right) p\left(x_{1}, \ldots, x_{n} ; \theta\right) dx.
$$
And the **Bayes risk** of estimator $\hat\theta(X)$ under prior is 
$$
B_{\pi}(\widehat{\theta})=\int R(\theta, \widehat{\theta}) \pi(\theta) d \theta.
$$
An estimator $\hat \theta$ minimizing the Bayes risk is called **Bayes estimator**, i,e., 
$$
B_{\pi}(\widehat{\theta})=\inf _{\widetilde{\theta}} B_{\pi}(\widetilde{\theta}).
$$
Next we define the **posterior risk** $r\left(\widehat{\theta} | x^{n}\right)$ of estimator $\hat \theta$ by
$$
r\left(\widehat{\theta} | x^{n}\right)=\int L\left(\theta, \widehat{\theta}\left(x^{n}\right)\right) \pi\left(\theta | x^{n}\right) d \theta.
$$
**Theorem 1.** *The Bayes risk satisfies that
$$
B_{\pi}(\widehat{\theta})=\int r\left(\widehat{\theta} | x^{n}\right) m\left(x^{n}\right) d x^{n},
$$
where $m\left(x^{n}\right)=\int p\left(x^{n} | \theta\right) \pi(\theta) d \theta$ is the marginal distribution of $X^n$. Let $\widehat{\theta}\left(x^{n}\right)$ be the value of $\theta$ that minimizes $r\left(\widehat{\theta} | x^{n}\right)$. Then $\widehat{\theta}\left(x^{n}\right)$ is the Bayes estimator.*

**Proof. **Note that
$$
\begin{align}
B_{\pi}(\widehat{\theta})&=\int R(\theta, \widehat{\theta}) \pi(\theta) d \theta\\
&= \int \int L\left(\theta, \widehat{\theta}\left(x^{n}\right)\right) p\left(x^n | \theta\right) dx^n \pi(\theta) d \theta\\
&=\int \int L\left(\theta, \widehat{\theta}\left(x^{n}\right)\right) p\left(x^n | \theta\right)  \pi(\theta) d \theta dx^n\\
&=\int \int L\left(\theta, \widehat{\theta}\left(x^{n}\right)\right)\pi\left(\theta | x^{n}\right)m\left(x^{n}\right)d \theta dx^n\\
&=\int r\left(\widehat{\theta} | x^{n}\right) m\left(x^{n}\right) d x^{n}.
\end{align}
$$
Moreover, there is an interesting fact about the relationship between unbiased estimator and Bayes estimator.

**Theorem 2.** *Consider the estimation of $g(\theta)$ when the loss function is square error. Then unbiased estimator $\delta(X)$ can be Bayes estimator unless
$$
\mathbb{E}\left[\delta(X)-g(\theta)\right]^2=0,
$$
where the expectation is taken with respective to both $X$ and $\theta$.*

**Proof. ** Assume $\delta(X)$ is an unbiased Bayes estimator, given the fact that the loss function is square error
$$
\delta(X)=\mathbb{E}\left[g(\theta)|X\right],\text{ a.s.}
$$
Since $\delta(X)$ is unbiased, 
$$
\mathbb{E}\left[\delta(X)|\theta\right]=g(\theta)
$$
for all $\theta$.
Then
$$
\mathbb{E}\left[\delta(X)g(\theta)\right]=\mathbb{E}\left[\mathbb{E}\left[\delta(X)g(\theta)\right|X]\right]=E\left[\delta^2(X)\right],
$$
and
$$
\mathbb{E}\left[\delta(X)g(\theta)\right]=\mathbb{E}\left[\mathbb{E}\left[\delta(X)g(\theta)\right|\theta]\right]=E\left[g(\theta)^2\right].
$$

# Hierarchical Bayes Estimator

The hierarchical Bayes model is
$$
\begin{aligned}
&X|\theta \sim f(x | \theta)\\
&\Theta|\gamma \sim \pi(\theta | \gamma)\\
&\Gamma \sim \psi(\gamma)
\end{aligned},
$$
given a loss function $L(\theta,d(X))$, we would then determine the estimator that minimizes 
$$
\int L(\theta, d(x)) \pi(\theta | x) d \theta
$$
where
$$
\pi(\theta | x)=\frac{\int f(x | \theta) \pi(\theta | \gamma) \psi(\gamma) d \gamma}{\iint f(x | \theta) \pi(\theta | \gamma) \psi(\gamma) d \theta d \gamma}.
$$
Note also that
$$
\pi(\theta | x)=\int \pi(\theta | x, \gamma) \pi(\gamma | x) d \gamma.
$$
Then we can write the posterior risk as
$$
\int L(\theta, d(x)) \pi(\theta | x) d \theta=\int\left[\int L(\theta, d(x)) \pi(\theta | x, \gamma) d \theta\right] \pi(\gamma | x) d \gamma.
$$
Sometimes it's difficult to obtain the posterior distroburion $\pi(\theta | x)$, but from hierarchical Bayes model, we can calculate the full conditionals
$$
\begin{aligned}
&\Theta|\mathbf{x}, \gamma \sim \pi(\theta | \mathbf{x}, \gamma)\\
&\Gamma|\mathbf{x}, \theta \sim \pi(\gamma | \mathbf{x}, \theta)
\end{aligned}.
$$
Then we can use **Markov chain Monte Carlo (MCMC)** algorithm, for $i=1,2,...,M$, random variables are generated according to
$$
\begin{aligned}
\Theta_{i} | \mathbf{x}, \gamma_{i-1} & \sim \pi\left(\theta | \mathbf{x}, \gamma_{i-1}\right) \\
\Gamma_{i} | \mathbf{x}, \theta_{i} & \sim \pi\left(\gamma | \mathbf{x}, \theta_{i}\right)
\end{aligned},
$$
which defines Markov chain $\left(\Theta_{i}, \Gamma_{i}\right)$. It follows from the theory of such chains that there exist distributions $\pi(\theta | \mathbf{x})$ and $\pi(\gamma | \mathbf{x})$ such that
$$
\begin{aligned}
\Theta_{i} \stackrel{\mathcal{L}}{\rightarrow} \Theta &\sim \pi(\theta | \mathbf{x})\\
\Theta_{i} \stackrel{\mathcal{L}}{\rightarrow} \Theta &\sim \pi(\theta | \mathbf{x})
\end{aligned}
$$
as $i \to \infty$. 
The Gibbs sampler actually yields two methods of calculating the same quantity.

  - As $M\to \infty$, 
$$
\frac{1}{M} \sum_{i=1}^{M} h\left(\Theta_{i}\right) \rightarrow E(h(\Theta) | \mathbf{x})=\int h(\theta) \pi(\theta | \mathbf{x}) d \theta.
$$

  - As $M\to \infty$,
$$
\frac{1}{M} \sum_{i=1}^{M} E h\left(\Theta | x, \Gamma_{i}\right) \rightarrow \int E(h(\Theta) | x, \gamma) \pi(\gamma | x) d \gamma=E(h(\Theta) | x).
$$

# Emperical Bayes

We can then write a Bayes model in a general form as
$$
\begin{aligned}
&X_i|\theta \sim f(x | \theta),\ i=1,2,...,n\\
&\Theta|\gamma \sim \pi(\theta | \gamma)
\end{aligned}
$$
but we now treat $\gamma$ as an unknown parameter of the model, which also needs to be estimated.  Calculate the marginal distribution of $X$ with density
$$
m(\mathbf{x} | \gamma)=\int \prod_{i=1}^n f\left(x_{i} | \theta\right) \pi(\theta | \gamma) d \theta,
$$
and based on $m(\mathbf{x} | \gamma)$ we can obtain MLE $\hat \gamma(\mathbf{x})$. Then determine the estimator that minimizes the empirical posterior loss
$$
\int L(\theta, \delta(\mathbf{x})) \pi(\theta | \mathbf{x}, \hat{\gamma}(\mathbf{x})) d \theta,
$$
and this minimizing estimator is the empirical Bayes estimator.

# Minimax Estimator

The **minimax risk** is defined by 
$$
R_{n}=\inf _{\widehat{\theta}} \sup _{\theta} R(\theta, \widehat{\theta}),
$$
and an estimator Î¸b is a minimax estimator if
$$
\sup _{\theta} R(\theta, \widehat{\theta})=\inf _{\widehat{\theta}} \sup _{\theta} R(\theta, \widehat{\theta}).
$$
**Theorem 3.** *Let $\hat \theta$ be the Bayes estimator for some prior $\pi$, if
$$
R(\theta, \widehat{\theta}) \leq B_{\pi}(\widehat{\theta})\  \text { for all } \theta
$$
then $\hat \theta$ is minimax and $\pi$ is called a least favorable prior.*

**Proof.** Suppose there is another estimator $\hat \theta_0$ satisfying
$$
\sup _{\theta} R\left(\theta, \widehat{\theta}_{0}\right)<\sup _{\theta} R(\theta, \widehat{\theta}).
$$
Then 
$$
B_{\pi}\left(\widehat{\theta}_{0}\right) \leq \sup _{\theta} R\left(\theta, \widehat{\theta}_{0}\right)<\sup _{\theta} R(\theta, \widehat{\theta}) \leq B_{\pi}(\widehat{\theta}),
$$
which is a contradiction. 

Applying this theorem, we have the following important result.

**Theorem 4.** Suppose that $\hat \theta$ is the Bayes estimator with respect to some prior $\pi$. If the risk is constant then $\hat \theta$ is minimax.

Another method to find minimax estimator is finding (upper and lower) bounds on the minimax risk that match. Then the estimator that achieves the upper bound is a minimax estimator.

For upper bound, we need to find an estimator satisfying
$$
\inf _{\tilde{\theta}} \sup _{\theta \in \Theta} R(\theta, \widetilde{\theta}) \leq R\left(\theta, \widehat{\theta}_{\mathrm{up}}\right).
$$
For lower bound, first we fix a prior $\pi$ and obtain the Bayes estimator $\widehat{\theta}_{\mathrm{low}}$, then we have
$$
B_{\pi}\left(\widehat{\theta}_{\mathrm{low}}\right) \leq B_{\pi}\left(\theta_{\text {minimax }}\right) \leq \sup _{\theta} R\left(\theta, \theta_{\text {minimax }}\right)=\inf _{\tilde{\theta}} \sup _{\theta \in \Theta} R(\theta, \widetilde{\theta}).
$$

Next we will find the minimax estimation of normal mean. Let $X = (X_1,...,X_n)$, with the $X_i$ iid according to $N(\theta,\sigma^2)$. First assume $\sigma^2$ is known, and take the conjugate normal distribution $N(\theta,b^2)$ as prior. Then the Bayes estimator of $\theta$ is
$$
\hat\theta(\mathbf{x})=\frac{n \bar{x} / \sigma^{2}+\mu / b^{2}}{n / \sigma^{2}+1 / b^{2}},
$$
and compute the risk
$$
R\left(\hat\theta(\mathbf{x}),\theta\right)=\mathbb{E}_{\mathbf{x}}\left[\hat\theta(\mathbf{x})-\theta\right]^2=\frac{n/\sigma^2+(\mu-\theta)^2/b^4}{(n / \sigma^{2}+1 / b^{2})^2}.
$$
Therefore the Bayes risk is
$$
B_{\pi}(\hat\theta(\mathbf{x}))=\mathbb{E}_{\theta}\left[R\left(\hat\theta(\mathbf{x}),\theta\right)\right]=\frac{1}{n / \sigma^{2}+1 / b^{2}},
$$
since $b$ is arbitrary, we can take the limit as $b\to \infty$ to obtain that the minimax risk is lower bounded by $\frac{\sigma^2}{n}$.

Note that, $\hat \theta=\bar X\sim N(\theta,\frac{\sigma^2}{n})$, then the risk is
$$
R(\bar X,\theta)=\mathbb{E}_X[\bar X-\theta]^2=\frac{\sigma^2}{n}.
$$
This implies that
$$
\inf _{\tilde{\theta}} \sup _{\theta \in \Theta} R(\theta, \widetilde{\theta}) \leq R(\theta, \widehat{\theta})=\frac{\sigma^2}{n},
$$
which is an upper bound. Therefore, the minimax estimation is $\bar X$.

If $\sigma^2$ is unknown, we assume that $\sigma^2$ is bounded by $M$, then we have
$$
\inf _{\tilde{\theta}} \sup _{\theta \in \Theta} R(\theta, \widetilde{\theta})\leq \frac{M}{n}.
$$
Note that, the minimax estimator does not, in fact, depend on the value of $M$.

