\documentclass[12pt]{article}
\usepackage[english]{babel}
\usepackage[utf8x]{inputenc}
\usepackage[colorinlistoftodos]{todonotes}
\usepackage{tocloft,lipsum,pgffor}

\setcounter{tocdepth}{2}% Include up to \subsubsection in ToC


\usepackage{hyperref}       % hyperlinks
\hypersetup{
	colorlinks=true,
	linkcolor=blue,
	filecolor=magenta,      
	urlcolor=cyan,
	citecolor = red
}
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{lipsum}		% Can be removed after putting your text content
\usepackage{amsmath,graphicx}
\usepackage[linesnumbered,boxed,ruled,commentsnumbered]{algorithm2e}%%算法包，注意设置所需可选项

%\renewcommand{\theequation}{\arabic{section}.\arabic{equation}}
\numberwithin{equation}{section}
% Use these for theorems, lemmas, proofs, etc.
\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{assumption}[theorem]{Assumption}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{claim}[theorem]{Claim}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{definition}[theorem]{Definition}
\newenvironment{proof}{{\bf Proof:}}{\hfill\rule{2mm}{2mm}}

% set page geometry
\usepackage[verbose=true,letterpaper]{geometry}
\AtBeginDocument{
	\newgeometry{
		textheight=9in,
		textwidth=6.5in,
		top=1in,
		headheight=14pt,
		headsep=25pt,
		footskip=30pt
	}
}

\widowpenalty=10000
\clubpenalty=10000
\flushbottom
%\sloppy

% float placement
\usepackage{natbib}
\bibliographystyle{abbrvnat}

\begin{document}
\title{Step into High-dimensional Statistics: Sparse Mean Estimation}

\author{Yajie Bao\thanks{Department of Mathematics, Shanghai Jiao Tong University, Eamil: baoyajie2019stat@sjtu.edu.cn}}
%\address{Shanghai Jiao Tong University}
%\email{baoyajie2019stat@sjtu.edu.cn}
\maketitle
Classical Statistics always has a basic assumption: $n>p$, and many estimation methods with good asymptotic properties were build based on this assumption. Many multivariate statistical models (see \citet{anderson1958introduction}) will fail when the number of variates is greater than sample size, such as linear regression, LDA, PCA... And in many situations, the dimension $p$ will increase with the growth of sample size $n$.

Take the normal mean estimation as an example, $X_i,\ i=1,2,...,n$ are i.i.d samples from multivariate normal distribution $N(\boldsymbol{\mu},\sigma^2 I_p)$, and sample mean $\boldsymbol{\bar X}$ is the minimax estimator of $\boldsymbol{\mu}$. Note that the minimax error is
$$
\mathbb{E}\left(\boldsymbol{\bar X}-\boldsymbol{\mu}\right)^2=\sum_{j=1}^{p}\mathbb{E}\left(\bar X_j-\mu_j\right)^2=\frac{p\sigma^2}{n},
$$ 
and obviously $\boldsymbol{\bar X}$ is not a consistent estimator when $n=o(p)$, which is called  the curse of dimensionality.

Another example is LDA, we need to compute the linear discriminant vector $\boldsymbol{\widehat{\Sigma}}\left(\boldsymbol{\bar X}-\boldsymbol{\bar Y}\right)$. The rank of sample covariance matrix is $\min\{n,p\}=n$, which means $\boldsymbol{\widehat{\Sigma}}$ is non-invertible. So we can't obtain $\boldsymbol{\widehat{\Sigma}}^{-1}$ directly.
\section{Gaussian sequence model}
A toy model in high-dimensional statistics is the Gaussian sequence model, 
\begin{equation}\label{1.1}
y_{ij} = \beta_j + z_{ij},\ i=1,2,...,n;\ j=1,2,...,p
\end{equation}
where $z_{ij}$ are i.i.d normal r.v with mean 0 and variance $\sigma^2$ for each $j$. Now we have $n$ observations for each $y_j$ to estimate $\boldsymbol{\beta}=(\beta_1,...,\beta_p)$. To overcome this problem, we need to add some assumptions on high-dimensional parameter $\boldsymbol{\beta}$. A direct thought is sparsity, i.e., there are only few non-zero elements in $\boldsymbol{\beta}$. And this assumption can be written as
\begin{equation}\label{1.2}
\sum_{j=1}^p1(|\beta_j|\neq 0)\leq s_0.
\end{equation}
Next step is to find the positions of non-zero parameter entries and obtain their estimation, and the first part of our goal is also called support recovery. If $\beta_j=0$, then $\hat{\beta_j}=\bar{Y_j}$ will be quite small. Thus we can only keep $\hat{\beta_j}$ with large magnitude, which leads to the idea of thresholding. 

There are many thresholding functions like hard thresholding, soft thresholding (see \citet{donoho1994ideal}), SCAD (see \citet{Fan2001}) etc. Here we use hard thresholding method
\begin{equation}
\widehat{\beta}_{j}= \bar{Y_j}\mathbb{I}\left(\left|\bar{Y_j}\right| \geq t\right), \quad \forall j \in\{1, \ldots, p\},
\end{equation}
where $\bar{Y_j}=\sum_{i=1}^ny_{ij}/n$. Next we will give some theoretical results on estimation error and support recovery. Before this we need a lemma on the bound of $\max _j\left|\bar Y_{j}-\beta_{j}\right|$
\begin{lemma}\label{lemma1}
	For the sample mean $\bar Y_j,\ j=1,2,...,p$
	\begin{equation}\label{1.4}
	\max _{i=1}^{p}\left|\bar Y_{j}-\beta_{j}\right| = O_p\left(\sqrt{\frac{\log p}{n}}\right).
	\end{equation}
\end{lemma}
\begin{proof}
	let $X_j=\bar Y_{j}-\beta_{j}=\frac{\sum_{i=1}^{n}z_{ji}}{n}$, where $z_{ji}\sim N(0,\sigma^2)$ and independent. Using the tail probability of normal random variables and the fact $X_j\sim N(0,\frac{\sigma^2}{n})$, we have
	\begin{align*}
	\mathbb{P}\left(\max _{j=1}^{p}\left|X_{j}\right|\geq t\right)&\leq \sum_{j=1}^{p}\mathbb{P}\left(\left|X_{j}\right|\geq t\right)\\
	&\leq 2p \exp\left(-\frac{nt^2}{2\sigma^2}\right).
	\end{align*}
	Set $t= \lambda\sqrt{\frac{\log p}{n}}$ for sufficiently large $\lambda$ and the result follows.
\end{proof}
\begin{theorem}[Support recovery]\label{theorem1.2}
	Let $S(\beta)=\{j:|\beta_{j}|\neq 0\}$ and $S(\widehat\beta)=\{j:|\widehat\beta_{j}|\neq 0\}$, assume that $\min_{j\in S}|\beta_{j}|>\sigma \sqrt{\frac{2 \log (2 p / \delta)}{n}}$ and set $t=\sigma \sqrt{\frac{2 \log (2 p / \delta)}{n}}$ then with probability at least $1-\delta$,
	\begin{equation}
	S(\beta)=S(\widehat{\beta}).
	\end{equation}
\end{theorem}
\begin{proof}
	According to the proof of Lemma \ref{lemma1}, with probability at least $1-\delta$,
	$$
	\max _{i=1}^{p}\left|\bar Y_{j}-\beta_{j}\right|\leq \sigma\sqrt{\frac{2 \log (2 p / \delta)}{n}}.
	$$
	If $j\in S$, then $|\widehat{\beta_j}|>0$, otherwise the error will be great than $\sigma\sqrt{\frac{2 \log (2 p / \delta)}{n}}$. If $j \in S^c$, then
	$$
	\mathbb{P}\left(|\widehat{\beta_j}|=0\right)= \mathbb{P}\left(\left|\bar Y_{j}-\beta_{j}\right|\geq t\right)\leq 1-\delta.
	$$
	Then we have completed the proof.
\end{proof}
\begin{theorem}[$\ell_1$ error bound]\label{theorem1.3}
	Under the assumption (\ref{1.2}) and set threshold $t=\lambda\sqrt{\frac{\log p}{n}}$
	\begin{equation}
	\|\boldsymbol{\widehat\beta}-\boldsymbol{\beta}\|_1=\sum_{j=1}^p|\widehat{\beta_j}-\beta_{j}|=O_p\left(s_0\sqrt{\frac{\log p}{n}}\right),
	\end{equation}
	where $\lambda> \sqrt{2}\sigma$.
\end{theorem}
\begin{proof}
	First using the assumption (\ref{1.2}) and Lemma \ref{lemma1}, we have
	\begin{align*}
	\left\|\boldsymbol{\widehat\beta}-\boldsymbol{\beta}\right\|_1&=\sum_{j=1}^p\left|\bar{Y_j}\mathbb{I}\left(\left|\bar{Y_j}\right| \geq t\right)-\beta_{j}\right|\\
	&= \sum_{j\in S}\left|\bar{Y_j}\mathbb{I}\left(\left|\bar{Y_j}\right| \geq t\right)-\beta_{j}\right|+\sum_{j\in S^c}\left|\bar{Y_j}\mathbb{I}\left(\left|\bar{Y_j}\right| \geq t\right)\right|\\
	&\leq  \sum_{j\in S}\left|\bar{Y_j}-\beta_{j}\right|+\sum_{j\in S}\left|\bar{Y_j}\right|\mathbb{I}\left(\left|\bar{Y_j}\right| < t\right)+\sum_{j\in S^c}\left|\bar{Y_j}\mathbb{I}\left(\left|\bar{Y_j}\right| \geq t\right)\right|\\
	&\leq s_0\max _{i=1}^{p}\left|\bar Y_{j}-\beta_{j}\right|+s_0t+\max _{i=1}^{p}\left|\bar Y_{j}-\beta_{j}\right|\sum_{j\in S^c}\mathbb{I}\left(\left|\bar{Y_j}\right| \geq t\right)\\
	&=O_p\left(s_0\sqrt{\frac{\log p}{n}}\right)+I.
	\end{align*}
	Then note that when $\beta_{j}=0$, $\bar{Y_j}\sim N(0,\frac{\sigma^2}{n})$ and
	\begin{align*}
	\mathbb{P}\left(\sum_{j\in S^c}\mathbb{I}\left(\left|\bar{Y_j}\right| \geq t\right)>0\right)&=\mathbb{P}\left(\max_{j\in S^c}|\bar{Y_j}|\geq t\right)\\
	&\leq 2p\exp\left(-\frac{\lambda^2}{2\sigma^2}\log p\right)\\
	&= 2\exp\left(-\frac{\lambda^2}{2\sigma^2}\log p+\log p\right)\to 0.
	\end{align*}
	Thus $I=o_p\left(s_0\sqrt{\frac{\log p}{n}}\right)$ and the result follows.
\end{proof}\newline
\textbf{Remark.} Through the analysis above, under the sparsity assumption (\ref{1.2}), if $s_0\sqrt{\frac{\log p}{n}}\to 0$ then hard thresholding estimator is still consistent.
\begin{theorem}[$\ell_{\infty}$ error bound]
	Under the assumption (\ref{1.2}) and set threshold $t=M_0\sqrt{\frac{\log p}{n}}$ for some $M_0>0$, then we have
	\begin{equation}
	\|\boldsymbol{\widehat\beta}-\boldsymbol{\beta}\|_{\infty}=O_p\left(\sqrt{\frac{\log p}{n}}\right).
	\end{equation}
\end{theorem}
\begin{proof}
	Note that there exists some $C_0$ such that,
	\begin{align*}
	\|\boldsymbol{\widehat\beta}-\boldsymbol{\beta}\|_{\infty}&\leq \max_{j=1}^p\left|\bar{Y_j}-\beta_{j}\right|+\max_{j=1}^p\left|\bar{Y_j}\right|\mathbb{I}\left(|\bar{Y_j}|<t\right)\\
	&\leq  C_0\sqrt{\frac{\log p}{n}}+t.
	\end{align*}
\end{proof}\newline
\textbf{Remark. }Using the simple norm inequality and Theorem \ref{theorem1.2}, we have $\ell_2$ error bound
\begin{equation}\label{1.8}
 \|\boldsymbol{\widehat\beta}-\boldsymbol{\beta}\|_{2}\leq \sqrt{s}\|\boldsymbol{\widehat\beta}-\boldsymbol{\beta}\|_{\infty}=O_p\left(\sqrt{\frac{s\log p}{n}}\right).
\end{equation}
 And according to \citet{Johnstone1986}, (\ref{1.8}) is statistical minimax lower bound of sparse mean estimation.
\section{New tail bound assumption}
Note that the assumption of normality is used to construct tail bound (\ref{1.4}), and this assumption can be substituted by the following condition:
\begin{assumption}[Exponential-type tails]\label{assumption2.1}
	Suppose that there exists some $\gamma>0$ such that
	\begin{equation}
	\mathrm{E} \exp \left(t z_{ij}^{2}\right) \leq K_{1}<\infty \quad \text { for all }|t| \leq \gamma \text { and } i, j
	\end{equation}
\end{assumption}
Here we use a lemma in \citet{Cai2011} as following:
\begin{lemma}
	Let $\xi_{1}, \dots, \xi_{n}$ be independent random variables with mean 0. Suppose that there exists some $\eta>0$ and $\bar{B}_{n}^{2}$ such that $\sum_{k=1}^{n} \mathrm{E} \xi_{k}^{2} e^{\eta\left|\xi_{k}\right|} \leq \bar{B}_{n}^{2}$. Then for $0<x \leq \bar{B}_{n}$, 
	\begin{equation}
	\mathrm{P}\left(\sum_{k=1}^{n} \xi_{k} \geq C_{\eta} \bar{B}_{n} x\right) \leq \exp \left(-x^{2}\right),
	\end{equation}
	where $C_{\eta}=\eta+\eta^{-1}$.
\end{lemma}
\begin{proof}
	By the inequality $\left|e^{s}-1-s\right| \leq s^{2} e^{|s|}$, we have for any $t\geq 0$,
	\begin{align*}
	\mathrm{P}\left(\sum_{k=1}^{n} \xi_{k} \geq C_{n} \bar{B}_{n} x\right) & \leq \exp \left(-t C_{\eta} \bar{B}_{n} x\right) \prod_{k=1}^{n} \mathrm{E} \exp \left(t \xi_{k}\right) \\
	& \leq \exp \left(-t C_{\eta} \bar{B}_{n} x\right) \prod_{k=1}^{n}\left(1+t^{2} \mathrm{E} \xi_{k}^{2} e^{t\left|\xi_{k}\right|}\right) \\
	& \leq \exp \left(-t C_{\eta} \bar{B}_{n} x+\sum_{k=1}^{n} t^{2} \mathrm{E} \xi_{k}^{2} e^{t\left|\xi_{k}\right|}\right).
	\end{align*}
	Take $t=\eta\left(x / \bar{B}_{n}\right)$, it follows that
	$$
	\mathrm{P}\left(\sum_{k=1}^{n} \xi_{k} \geq C_{\eta} \bar{B}_{n} x\right) \leq \exp \left(-\eta C_{\eta} x^{2}+\eta^{2} x^{2}\right)=\exp \left(-x^{2}\right).
	$$
\end{proof}
\begin{theorem}
	Assume that the noise $z_{ij}$ satisfying Assumption \ref{assumption2.1}, we have
	\begin{equation}
	\max _{i=1}^{p}\left|\bar Y_{j}-\beta_{j}\right| = O_p\left(\sqrt{\frac{\log p}{n}}\right).
	\end{equation}
\end{theorem}
\begin{proof}
	Using the simple inequality 
	$$
	s^{2} e^{s} \leq e^{2 s}\leq e^{s^2+1},
	$$
	we have for each $i,\ j$
	$$
	\mathrm{E}\left(z_{ij}^2e^{\eta |z_{ij}|}\right)\leq \mathrm{E}\left(\eta^{-2}\exp(2\eta|z_ij|)\right)\leq e\mathrm{E}\left(\eta^{-2}\exp(\eta^2|z_ij|^2)\right).
	$$
	By Assumption \ref{assumption2.1}, we can set 
	$$
	\bar{B}_{n}^2=ne\eta^{-2} K_1,
	$$
	where $0<\eta<\sqrt{\gamma}$. Then for sufficiently large $\eta$
	\begin{align*}
	\mathrm{P}\left(\max _{i=1}^{p}\left|\bar Y_{j}-\beta_{j}\right|>C\sqrt{\frac{\log p}{n}}\right)&\leq \sum_{j=1}^p\mathrm{P}\left(\sum_{i=1}^n|z_{ij}|>C\sqrt{n\log p}\right)\\
	&=p\mathrm{P}\left(\sum_{i=1}^n|z_{ij}|>C\bar{B}_ne^{-1}\eta K_1^{-\frac{1}{2}}\sqrt{\log p}\right)\\
	&\to 0,
	\end{align*}
	whcih completes the proof.
\end{proof}\newline
\textbf{Remark.} Assumption \ref{assumption2.1} is very similar to sub-Gaussian (see \citet{vershynin2018high}), which has tail
\begin{equation}\label{2.4}
\mathbb{P}\{|X| \geq t\} \leq 2 \exp \left(-t^{2} / K_{1}^{2}\right) \quad \text { for all } t \geq 0.
\end{equation}
And there is concentration inequality about sum of independent sub-Gaussian random variables.
\begin{theorem}[General Hoeffding’s inequality]
	Let $X_i,\ i=1,2,...,N$ be be independent, mean zero, sub-gaussian random variables with parameter $\sigma_i$, then for every $t\geq 0$,
	\begin{equation}
	\mathbb{P}\left\{\left|\sum_{i=1}^{N} X_{i}\right| \geq t\right\} \leq 2 \exp \left(-\frac{c t^{2}}{\sum_{i=1}^{N}\sigma_i^2}\right).
	\end{equation}
\end{theorem}
Besides Exponential-type tails, there is another common tail called Polynomial-type tails.
\begin{assumption}[Polynomial-type tails]\label{assumption2}
	Suppose that for some $\gamma>0$, 
	\begin{equation}
	E\left|z_{i j}\right|^{2(1+\gamma)} \leq K \quad \text { for all } i, j.
	\end{equation}
\end{assumption}
\begin{theorem}
	Under the Assumption (\ref{assumption2}), we have
	\begin{equation}
	\max _{i=1}^{p}\left|\bar Y_{j}-\beta_{j}\right| = O_p\left(\frac{p^{1/2(1+\gamma)}}{n^{1/2}}\right).
	\end{equation}
\end{theorem}
\begin{proof}
	We use a moment inequality in \citet{shao2003mathematical}, for $q>0$
	\begin{equation}
	E\left|\sum_{i=1}^{n} z_{ij}\right|^{q} \leq \frac{C_{q}}{n^{1-q / 2}} \sum_{i=1}^{n} E\left|X_{i}\right|^{q}.
	\end{equation}
	By Markov inequality,
	\begin{align*}
	\mathrm{P}\left(\max _{i=1}^{p}\left|\bar Y_{j}-\beta_{j}\right|>t\right)&\leq p\frac{\mathrm{E}\left|\sum_{i=1}^nz_{ij}\right|^{2(1+\gamma)}}{(nt)^{2(1+\gamma)}}\\
	&\leq p\frac{Cn^{1+\gamma}K_2}{(nt)^{2(1+\gamma)}}\\
	&=pC_pK_2n^{-(1+\gamma)}t^{-2(1+\gamma)}.
	\end{align*}
	Let $t=M\frac{p^{1/2(1+\gamma)}}{n^{1/2}}$ for sufficiently large $M$, then we complete the proof.
\end{proof}\newline
\textbf{Remark.} If we take threshold $t=M\frac{p^{1/2(1+\gamma)}}{n^{1/2}}$, then the convergence rate of $\ell_1$ error will be $O_p(s_0\frac{p^{1/2(1+\gamma)}}{n^{1/2}})$.
\section{New sparsity assumption}
Sparsity assumption (\ref{1.2}) is actually an $\ell_0$ ball in $\mathbb{R}^p$, which can be genlized to $\ell_q$ ball in $\mathbb{R}^p$, i.e., for $0\leq q<1$
\begin{equation}
\mathcal{U}\left(q,s_q\right)=\left\{\boldsymbol{\beta}\in \mathbb{R}^p:\sum_{j=1}^p|\beta_{j}|^q\leq s_q\right\}.
\end{equation}
Next we will build convergence rate of $\ell_q$, and the proof is very similar to the Theorem 1 in \citet{Bickel2008thres}.
\begin{theorem}[$\ell_1$ error bound]
	If $\beta\in \mathcal{U}\left(q,s_q\right)$ and set threshold $t_n=M\sqrt{\frac{\log p}{n}}$ for sufficiently large $M$. Suppose thta noise $z_{ij}$ are sub-Gaussian random variables with same parameter $\sigma$, then
	\begin{equation}\label{3.2}
	\left\|\boldsymbol{\widehat\beta}-\boldsymbol{\beta}\right\|_1=\sum_{j=1}^p|\widehat{\beta_j}-\beta_{j}|=O_p\left(s_q\left(\frac{\log p}{n}\right)^{(1-q)/2}\right).
	\end{equation}
\end{theorem}
\begin{proof}
	Let $T_{t_n}$ be hard thresholding function with threshold $t_n$, then note that
	\begin{equation}\label{3.3}
		\left\|\boldsymbol{\widehat\beta}-\boldsymbol{\beta}\right\|_1\leq 	\left\|T_{t_n}\left(\boldsymbol{\bar Y}\right)-T_{t_n}\left(\boldsymbol{\beta}\right)\right\|_1+\left\|\boldsymbol{\beta}-T_{t_n}\left(\boldsymbol{\beta}\right)\right\|_1.
	\end{equation}
	By $\beta\in \mathcal{U}\left(q,s_q\right)$ we have
	\begin{align*}
		\left\|\boldsymbol{\beta}-T_{t_n}\left(\boldsymbol{\beta}\right)\right\|_1&=\sum_{j=1}^p\left|\beta_{j}-\beta_{j}\mathbb{I}\left(|\beta_{j}|\geq t_n\right)\right|\\
		&=\sum_{j=1}^p\left|\beta_{j}\right|\mathbb{I}\left(|\beta_{j}|< t_n\right)\\
		&\leq \sum_{j=1}^p\left|\beta_{j}\right|^qt_n^{1-q}\mathbb{I}\left(|\beta_{j}|< t_n\right)\\
		&\leq s_qt_n^{1-q}.
	\end{align*}
	Next we will bound the first term of (\ref{3.3}),
	\begin{align*}
	\left\|T_{t_n}\left(\boldsymbol{\bar Y}\right)-T_{t_n}\left(\boldsymbol{\beta}\right)\right\|_1
	&\leq \sum_{j=1}^p\left|\bar{Y_j}\right|\mathbb{I}\left(|\bar{Y_j}|\geq t_n,\ |\beta_{j}|< t_n\right)\\
	&+\sum_{j=1}^p\left|\bar{Y_j}-\beta_{j}\right|\mathbb{I}\left(|\bar{Y_j}|\geq t_n,\ |\beta_{j}|\geq t_n\right)\\
	&+\sum_{j=1}^p\left|\beta_{j}\right|\mathbb{I}\left(|\bar{Y_j}|< t_n,\ |\beta_{j}|\geq t_n\right)\\
	&=\mathrm{I}+\mathrm{II}+\mathrm{III}.
	\end{align*}
	For the second term, there exists some $C_1>0$ such that,
	\begin{align*}
	\mathrm{II}&\leq \sum_{j=1}^p\left|\bar{Y_j}-\beta_{j}\right|\mathbb{I}\left(\ |\beta_{j}|\geq  t_n\right)\\
	&\leq \max_{j=1}^p\left|\bar{Y_j}-\beta_{j}\right|\sum_{j=1}^p\mathbb{I}\left(\ |\beta_{j}|\geq t_n\right)\\
	&\leq C_1\sqrt{\frac{\log p}{n}}s_qt_n^{-q}.
	\end{align*}
	For the third term,
	\begin{align*}
	\mathrm{II}&\leq \sum_{j=1}^p\left|\beta_{j}-\bar{Y_j}\right|\mathbb{I}\left( |\beta_{j}|\geq t_n\right)+t_n\sum_{j=1}^p\mathbb{I}\left(|\beta_{j}|\geq t_n\right)\\
	&\leq C_1\sqrt{\frac{\log p}{n}}s_qt_n^{-q}+s_qt_n^{1-q}.
	\end{align*}
	For the first term,
	\begin{align*}
	\mathrm{I}&\leq \sum_{j=1}^p\left|\bar{Y_j}-\beta_{j}\right|\mathbb{I}\left(|\bar{Y_j}|\geq t_n,\ |\beta_{j}|< t_n\right)+\sum_{j=1}^p\left|\beta_{j}\right|\mathbb{I}\left(|\bar{Y_j}|\geq t_n,\ |\beta_{j}|< t_n\right)\\
	&\leq \sum_{j=1}^p\left|\bar{Y_j}-\beta_{j}\right|\mathbb{I}\left(|\bar{Y_j}|\geq t_n,\ |\beta_{j}|< t_n\right)+s_qt_n^{1-q}\\
	&=\mathrm{IV}+s_qt_n^{1-q}.
	\end{align*}
	Now take $\gamma\in (0,1)$,
	\begin{align*}
	\mathrm{IV}&=\sum_{j=1}^p\left|\bar{Y_j}-\beta_{j}\right|\mathbb{I}\left(|\bar{Y_j}|\geq t_n,\ |\beta_{j}|< \gamma t_n\right)+\sum_{j=1}^p\left|\bar{Y_j}-\beta_{j}\right|\mathbb{I}\left(|\bar{Y_j}|\geq t_n,\ \gamma t_n\leq |\beta_{j}|\leq t_n\right)\\
	&\leq \sum_{j=1}^p\left|\bar{Y_j}-\beta_{j}\right|\mathbb{I}\left(|\bar{Y_j}|\geq t_n,\ |\beta_{j}|< \gamma t_n\right)+\sum_{j=1}^p\left|\bar{Y_j}-\beta_{j}\right|\mathbb{I}\left(|\beta_{j}|\geq \gamma t_n\right)\\
	&\leq C_1\sqrt{\frac{\log p}{n}}\sum_{j=1}^p\mathbb{I}\left(\left|\bar{Y_j}-\beta_{j}\right|>(1-\gamma)t_n\right)+C_1\sqrt{\frac{\log p}{n}}s_q(\gamma t_n)^{-q},\\
	\end{align*}
	moreover using (\ref{2.4}) and make $(1-\gamma)^2M>2\sigma^2$ we have
	\begin{align*}
	\mathrm{P}\left(\sum_{j=1}^p\mathbb{I}\left(\left|\bar{Y_j}-\beta_{j}\right|>(1-\gamma)t_n\right)>0\right)&=\mathrm{P}\left(\max_{j=1}^p|\bar{Y_j}-\beta_{j}|>(1-\gamma)t_n\right)\\
	&\leq p\exp\left(-\frac{(1-\gamma)^2M\log p}{2\sigma^2}\right)\\
	&=\exp\left(\log p-\frac{(1-\gamma)^2M}{2\sigma^2}\log p\right)\\
	&\to 0.
	\end{align*}
	Combining the inequalities above, (\ref{3.2}) is proved.
\end{proof}
\bibliography{docbib}
\end{document}