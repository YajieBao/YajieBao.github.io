\documentclass[12pt]{article}
\usepackage[english]{babel}
\usepackage[utf8x]{inputenc}
\usepackage[colorinlistoftodos]{todonotes}
\usepackage{tocloft,lipsum,pgffor}

\setcounter{tocdepth}{2}% Include up to \subsubsection in ToC


\usepackage{hyperref}       % hyperlinks
\hypersetup{
	colorlinks=true,
	linkcolor=blue,
	filecolor=magenta,      
	urlcolor=cyan,
	citecolor = red
}
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{lipsum}		% Can be removed after putting your text content
\usepackage{amsmath,graphicx}
\usepackage[linesnumbered,boxed,ruled,commentsnumbered]{algorithm2e}%%算法包，注意设置所需可选项

%\renewcommand{\theequation}{\arabic{section}.\arabic{equation}}
\numberwithin{equation}{section}
% Use these for theorems, lemmas, proofs, etc.
\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{assumption}[theorem]{Assumption}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{claim}[theorem]{Claim}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{definition}[theorem]{Definition}
\newenvironment{proof}{{\bf Proof:}}{\hfill\rule{2mm}{2mm}}

% set page geometry
\usepackage[verbose=true,letterpaper]{geometry}
\AtBeginDocument{
	\newgeometry{
		textheight=9in,
		textwidth=6.5in,
		top=1in,
		headheight=14pt,
		headsep=25pt,
		footskip=30pt
	}
}

\widowpenalty=10000
\clubpenalty=10000
\flushbottom
%\sloppy

% float placement
\usepackage{natbib}
\bibliographystyle{abbrvnat}

\begin{document}
\title{Theoretical Results of Lasso Solutions}

\author{Yajie Bao\thanks{Department of Mathematics, Shanghai Jiao Tong University, Eamil: baoyajie2019stat@sjtu.edu.cn}}
%\address{Shanghai Jiao Tong University}
%\email{baoyajie2019stat@sjtu.edu.cn}
\maketitle

\section{Single variable Lasso}
Consider single variable Lasso problem
\begin{equation}
\underset{\beta}{\operatorname{minimize}}\left\{\frac{1}{2 N} \sum_{i=1}^{N}\left(y_{i}-z_{i} \beta\right)^{2}+\lambda|\beta|\right\}
\end{equation}
which is a convex problem. By optimal condition of convex optimization problem, set the subgradient be 0 then we have
$$
\widehat{\beta}=\left\{\begin{array}{ll}{\frac{1}{N}\langle\mathbf{z}, \mathbf{y}\rangle-\lambda} & {\text { if } \frac{1}{N}\langle\mathbf{z}, \mathbf{y}\rangle} \\ { 0} & {\text { if } \frac{1}{N}|\langle\mathbf{z}, \mathbf{y}\rangle| \leq \lambda} \\ {\frac{1}{N}\langle\mathbf{z}, \mathbf{y}\rangle+\lambda} & {\text { if } \frac{1}{N}\langle\mathbf{z}, \mathbf{y}\rangle<-\lambda}\end{array}\right.
$$
which can be write as
$$
\widehat{\beta}=\mathcal{S}_{\lambda}\left(\frac{1}{N}\langle\mathbf{z}, \mathbf{y}\rangle\right),
$$
where $\mathcal{S}_{\lambda}(x) = \operatorname{sign}(x)(|x|-\lambda)_{+}$ is the \textit{soft-thresholding operator}.
\section{Uniqueness of Lasso solution}\label{Uniqueness of Lasso solution}
Now given a response vector $y\in \mathbb{R}^n$, a design matrix $X \in \mathbb{R}^{n\times p}$ and a tuning parameter $\lambda\geq 0$, the lasso estimate is
\begin{equation}\label{7.2}
\hat{\beta} \in \underset{\beta \in \mathbb{R}^{p}}{\operatorname{argmin}} \frac{1}{2}\|y-X \beta\|_{2}^{2}+\lambda\|\beta\|_{1}.
\end{equation}
\citet*{tibshirani2013lasso} gave some basic propeties about the lasso solutions, which is called the uniqueness of fitted values.
\begin{lemma}\label{lemma 7.1}
	For any $y, X$ and $\lambda\geq 0$, the lasso problem (\ref{7.2}) have the following properties:
	\begin{enumerate}
		\item Every lasso solution gives the same fitted value $X\hat{\beta}$
		\item If $\lambda >0$, then every lasso solution has the same $\ell_1$ norm $\|\hat{\beta}\|_1$.
	\end{enumerate}
\end{lemma}
\begin{proof}
	Suppose there are two different solutions $\hat{\beta^{(1)}}$ and $\hat{\beta^{(2)}}$, and denote the minimum value by $c^{*}$. By the strong convexity of $(y-x)^2$ and convexity of $\ell_1$ norm, for $0<\alpha<1$ we have
	$$
	\frac{1}{2}\left\|y-X\left(\alpha \hat{\beta}^{(1)}+(1-\alpha) \hat{\beta}^{(2)}\right)\right\|_{2}^{2}+\lambda\left\|\alpha \hat{\beta}^{(1)}+(1-\alpha) \hat{\beta}^{(2)}\right\|_{1}<\alpha c^{*}+(1-\alpha) c^{*}=c^{*},
	$$
	which leads a contradiction. Then we have proved assertion 1, it follows assertion 2 immediately.
\end{proof}

By KTT conditions, we have the following results
\begin{equation}\label{7.3}
X^{T}(y-X \hat{\beta})=\lambda \gamma,
\end{equation}
$$
\gamma_{i} \in\left\{\begin{array}{ll}{\left\{\operatorname{sign}\left(\hat{\beta}_{i}\right)\right\}} & {\text { if } \hat{\beta}_{i} \neq 0} \\ {[-1,1]} & {\text { if } \hat{\beta}_{i}=0}\end{array}, \quad \text { for } i=1, \ldots p\right.
$$
Here $\gamma$ is the subgradient of $\|\beta\|_1$ evaluated at $\hat\beta$.  First we define the equicorrelation set 
$$S = \left\{i \in\{1, \ldots p\}:\left|X_{i}^{T}(y-X \hat{\beta})\right|=\lambda\right\},$$
and by (\ref{7.3}) and the uniqueness of fitted value, every lasso optimal solution has the same subgradient. \textbf{If there exist a optimal solution $\hat{\beta}$ such that its subgradient satisfies $|\gamma_{i}|<1$ for any $i\notin S$}, then the lasso problem (\ref{7.2}) has the unique equicorrelation set for fixed $\lambda>0$.

There are some other equivalent forms of Lasso problem (\ref{7.2}),
$$
\min_{\beta \in \mathbb R^p}\|y-X\beta\|_2^2\quad \text{subject to } \|\beta\|_1\leq \lambda_s,
$$
and
$$
\min_{\beta \in \mathbb R^p}\|\beta\|_1\quad \text{subject to } \|y-X\beta\|_2^2\leq \gamma_s.
$$
\citet*{candes2007} proposed Dantzig selector as
\begin{equation}\label{dantzig}
\min_{\beta \in \mathbb R^p}\|\beta\|_1 \quad \text{subject to } \|X^T(y-X\beta)\|_{\infty}\leq \lambda_D.
\end{equation}
Let $\tilde{\beta}$ be the optimal solution of Lasso program (\ref{7.2}), by the zero-subgradient condition, it's easy to see that $\tilde{\beta}$ is a feasible solution of (\ref{dantzig}) when $\lambda_D=\lambda$.
\section{Lasso dual}
Given $y \in \mathbb{R}^{n}, X \in \mathbb{R}^{n \times p}$, recall the Lasso problem
$$
\min _{\beta \in \mathbb{R}^{p}} \frac{1}{2}\|y-X \beta\|_{2}^{2}+\lambda\|\beta\|_{1}.
$$
We can transform the primal to 
$$
\min _{\beta \in \mathbb{R}^{p}, z \in \mathbb{R}^{n}} \frac{1}{2}\|y-z\|_{2}^{2}+\lambda\|\beta\|_{1} \quad \text { subject to } \quad z=X \beta.
$$
Then the dual function is
$$
g(u)=\min _{\beta \in \mathbb{R}^{p}, z \in \mathbb{R}^{n}} \frac{1}{2}\|y-z\|_{2}^{2}+\lambda\|\beta\|_{1}+u^{T}(z-X \beta),
$$
taking derivatives on $z$ and $beta$, according to optimal condion we have
\begin{align*}
z&=y-u\\
\lambda \partial\|\beta\|_1&=X^Tu.
\end{align*}
This yields the lasso dual problem
$$
\max _{u \in \mathbb{R}^{n}} \frac{1}{2}\left(\|y\|_{2}^{2}-\|y-u\|_{2}^{2}\right) \quad \text { subject to }\left\|X^{T} u\right\|_{\infty} \leq \lambda,
$$
or equivalently
$$
\min _{u \in \mathbb{R}^{n}}\|y-u\|_{2}^{2}\quad \text { subject to }\left\|X^{T} u\right\|_{\infty} \leq \lambda.
$$
Further, note that given the dual solution $u$, any lasso solution $\beta$ satisfies
$$
X \beta=y-u,
$$
so \textbf{the lasso fit is just the dual residual}.
\section{$\ell_2$ bound}
Next we will establish the $\ell_2$ bound for $\left|\hat{\beta}-\beta^{*}\right|$. First we will rewrite the Lasso program as
\begin{equation}
\hat\beta=\text{arg}\min_{\beta \in \mathbb{R}^p}\frac{1}{2}\beta^TA\beta-b^T\beta+\lambda\|\beta\|_1,
\end{equation}
where $A=\frac{1}{n}\sum_{i=1}^{n}X_iX_i^T$ and $b=\frac{1}{n}\sum_{i=1}^{n}y_iX_i^T$.
\begin{theorem}
	Suppose that there exists some positive constants $c_1$ and $c_2$ such that 
	\begin{equation}
	\min_{\delta:\|\delta\|_1<c_1\sqrt{s}\|\delta\|_2}\frac{\delta^TA\delta}{\|\delta\|_2^2}\geq c_2,
	\end{equation}\label{7.16}
	and the regularization parameter satisfies $\|A\beta^{*}-b\|_{\infty}\leq \frac{\lambda}{2}$. Then there exists some positive constant $c$ such that
	\begin{equation}
	\|\hat{\beta}-\beta^{*}\|_2\leq c\sqrt{s}\lambda,
	\end{equation}
	where $k=|S|$.
\end{theorem}
\begin{proof}
	First we need to verify the scale of regularization parameter, note that
	\begin{align*}
	\|A\beta^{*}-b\|_{\infty}&=\left\|\frac{1}{n}\sum_{i=1}^{n}X_iw_i\right\|_{\infty}=\max_{1\leq j\leq p}\left|\sum_{i=1}^{n}X_{ij}w_i\right|.
	\end{align*}
	Then by exponential inequality, there exists some positive constant $C$ such that
	$$
	\lambda= C\sqrt{\frac{\log p}{n}},
	$$
	and $\|A\beta^{*}-b\|_{\infty}\leq \frac{\lambda}{2}$ holds with probability 1 for sufficiently large $n$. Next we will prove $\|\hat{\beta}-\beta^{*}\|_1\leq 4\sqrt{s}\|\hat{\beta}-\beta^{*}\|_2$. Using the fact that $\hat{\beta}$ is the optimal solution of the Lasso program, then we obtain
	\begin{equation}
	\frac{1}{2}\hat\beta^TA\beta-b^T\hat\beta+\lambda\|\hat\beta\|_1\leq \frac{1}{2}\beta^{*T}A\beta^{*}-b^T\beta^{*}+\lambda\|\beta^{*}\|_1,
	\end{equation}
	which implies that 
	\begin{align*}
	\frac{1}{2}\hat\beta^TA\beta-b^T\hat\beta-\left(\frac{1}{2}\beta^{*T}A\beta^{*}-b^T\beta^{*}\right)&\leq \lambda\left(\|\beta^{*}\|_1-\|\hat\beta\|_1\right)\\
	&=\lambda\left(\|\beta^{*}_S\|_1-\|\hat\beta_S\|_1-\|\hat\beta_{S^c}\|_1\right)\\
	&\leq \lambda\left(\|\beta^{*}_S-\hat\beta_S\|_1-\|\beta^{*}_{S^c}-\hat\beta_{S^c}\|_1\right).
	\end{align*}
	Note that,
	\begin{align*}
	\frac{1}{2}\hat\beta^TA\beta-b^T\hat\beta-\left(\frac{1}{2}\beta^{*T}A\beta^{*}-b^T\beta^{*}\right)&\geq (A\beta^{*}-b)^T(\hat{\beta}-\beta^{*})\\
	&\geq -\left\|A\beta^{*}-b\right\|_{\infty}\|\hat{\beta}-\beta^{*}\|_1\\
	&\geq -\frac{\lambda}{2}\|\hat{\beta}-\beta^{*}\|_1\\
	&=-\frac{\lambda}{2}\left(\|\hat{\beta}_S-\beta^{*}_S\|_1+\|\hat{\beta}_{S^c}-\beta^{*}_{S^c}\|_1\right)
	\end{align*}
	and combining the inqualities above we have
	\begin{equation}
	\|\hat{\beta}_{S^c}-\beta^{*}_{S^c}\|_1\leq 3 \|\hat{\beta}_S-\beta^{*}_S\|_1.
	\end{equation}
	Thus we can obtain
	\begin{align*}
	\|\hat{\beta}-\beta^{*}\|_1&=\|\hat{\beta}_{S}-\beta^{*}_{S}\|_1+\|\hat{\beta}_{S^c}-\beta^{*}_{S^c}\|_1\\
	&\leq 4\sqrt{s}\|\hat{\beta}_{S}-\beta^{*}_{S}\|_2.
	\end{align*}
	Then by assumption (\ref{7.16}), we have
	$$
	\left(\hat{\beta}-\beta^{*}\right)^TA\left(\hat{\beta}-\beta^{*}\right)\geq c_2\|\hat{\beta}-\beta^{*}\|_2.
	$$
	In addition, the zero-subgradient conditon implies $\|A\hat{\beta}-b\|_{\infty}\leq \lambda$, then
	\begin{align*}
	\left(\hat{\beta}-\beta^{*}\right)^TA\left(\hat{\beta}-\beta^{*}\right)&\leq \left(\hat{\beta}-\beta^{*}\right)^T\left(A\hat{\beta}-b-(A\beta^{*}-b)\right)\\
	&\leq \frac{3}{2}\lambda\|\hat{\beta}-\beta^{*}\|_1.
	\end{align*}
	Thus $\|\hat{\beta}-\beta^{*}\|_2\leq 6\sqrt{s}\lambda$.
\end{proof}
\section{Primal-Dual Witness Construction}
For any vector $\beta \in \mathbb{R}^p$, we define its support $S(\beta)=\left\{i | \beta_{i} \neq 0\right\}$. Then we begin with a lemma in  \citet*{wainwright2009sharp}.
\begin{lemma}\label{lemma 7.2}
	\begin{enumerate}
		\item A vector is optimal if and only if there exists a subgradient vector $\hat z\in \partial |\hat{\beta}|_1$ satisfying
		\begin{equation}\label{7.4}
		\frac{1}{n} X^{T} X\left(\widehat{\beta}-\beta^{*}\right)-\frac{1}{n} X^{T} w+\lambda_{n} \widehat{z}=0.
		\end{equation}
		\item Suppose the subgradient vector satisfies the strict dual feasibility condition $|\widehat{z_j}|<1$ for any $j \notin S(\hat{\beta})$. Then any optimal solution $\tilde{\beta}$ of lasso program satisfies $\tilde{\beta}_j=0$ for any $j \notin S(\hat{\beta})$.
		
		\item Under the condition of part (2), if the $k\times k$ matrix $X_{S(\hat{\beta})}^{T} X_{S(\hat{\beta})}$ is invertible, then $\hat{\beta}$ is the unique optimal solution of the Lasso program.
	\end{enumerate}
\end{lemma}
\begin{proof}
	The assertion 1 and 2 follows KKT conditions and uniqueness of subgradient in section \ref{Uniqueness of Lasso solution}. 
	
	Under the condition of part (2), the lasso program can be restricted in subspace $\beta \in \mathbb{R}^S$. If $X_{S(\hat{\beta})}^{T} X_{S(\hat{\beta})}$ is invertible, then the lasso program is strictly convex, therefore its optimal solution is unique.
\end{proof}

Let $S$ be the support set of the ture vector $\beta^{*}$, and assume that $X_{S}^{T} X_{S}$ is invertible. \citet*{wainwright2009sharp} proposed an incredible method called \textit{Primal-Dual Witness Construction}, which conscruted an lasso solution covering ture support set under strict dual feasibility condition. The steps of \textit{Primal-Dual Witness Construction} are presented as following
\begin{enumerate}
	\item First, we obtain $\check{\beta}_{S} \in \mathbb{R}^{k}$ by solving restricted Lasso problem
	\begin{equation}
	\check{\beta}_{S}=\arg \min _{\beta_{S} \in \mathbb{R}^{k}}\left\{\frac{1}{2 n}\left\|y-X_{S} \beta_{S}\right\|_{2}^{2}+\lambda_{n}\left\|\beta_{S}\right\|_{1}\right\}.
	\end{equation}
	And the solution is unqiue under the condition that $X_{S}^{T} X_{S}$ is invertible. We set $\check{\beta}_{S^c}=0$.
	\item Second, we choose an subgradient of the $\ell_1$ norm of $\check{\beta}_{S}$ denoted by $\check{z}_{S} \in \mathbb{R}^{k}$.
	\item Third, we obtain $\check{z}_{S^c}$ by solving the zero-subgradient condition (\ref{7.4}). Then we check for strict dual feasibility, i.e. $|\check{z}_{S^c}|<1$ for all $j\in S^c$.
	\item Fourth, we check whether the sign consistency condition $\check{z}_{S}=\operatorname{sign}\left(\beta_{S}^{*}\right)$ is satisfied.
\end{enumerate}

Since the pair $(\check{\beta},\ \check{z})$ satisfies the zero-subgradient condition (\ref{7.4}), it must be an optimal solution of the Lasso program. Then according to Lemma \ref{lemma 7.2}, it's easy to see that if steps 1-3 succeed with strict dual feasibulity, then the Lasso program has a unqiue solution with $S(\hat{\beta}) \subseteq S\left(\beta^{*}\right)$; If steps 1-4 succeed with strict dual feasibulity, then  the Lasso program has a unqiue solution with the correct signed support. Next we will prove strict dual feasibility and sign consistency condition.

We can write zero-subgradient condition (\ref{7.4}) as matrix form,
\begin{equation}
\frac{1}{n}\left[\begin{array}{cc}{X_{S}^{T} X_{S}} & {X_{S}^{T} X_{S c}} \\ {X_{S^{c}}^{T} X_{S}} & {X_{S^{c}}^{T} X_{S^{c}}}\end{array}\right]\left[\begin{array}{c}{\beta_{S}-\beta_{S}^{*}} \\ {0}\end{array}\right]-\frac{1}{n}\left[\begin{array}{l}{X_{S}^{T}} \\ {X_{S^{c}}^{T}}\end{array}\right]\left[\begin{array}{c}{w_{S}} \\ {w_{S^{c}}}\end{array}\right]+\lambda\left[\begin{array}{c}{z_{S}} \\ {z_{S^{c}}}\end{array}\right]=\left[\begin{array}{l}{0} \\ {0}\end{array}\right],
\end{equation}
the element of noise vector $w$ is sub-gaussian variable with zero-mean and parameter $\sigma^2$. Using the assumed invertibility of $X_{S}^{T} X_{S}$ we can solve for $\check{\beta}_{S}-\beta_{S}^{*}$ as follows
\begin{equation}
\check{\beta}_{S}-\beta_{S}^{*}=\left(\frac{1}{n} X_{S}^{T} X_{S}\right)^{-1}\left[\frac{1}{n} X_{S}^{T} w-\lambda \check{z}_{S}\right].
\end{equation}
Moreover, we can solve $\check{z}_{S^c}$ in terms of $\check{\beta}_{S}-\beta_{S}^{*}$ as
\begin{equation}\label{7.8}
\hat{z}_{S^{c}}=X_{S^{c}}^{T}\left[X_{S}\left(X_{S}^{T} X_{S}\right)^{-1} \check{z}_{S}+\Pi_{X_{S}^{\perp}}\left(\frac{w}{\lambda_{n} n}\right)\right],
\end{equation}
where $\Pi_{X_{S}^{\perp}}=I_S-\left(X_{S}^{T} X_{S}\right)^{-1}X_S^T$ is orthogonal projection matrix. We define the random variable 
\begin{equation}\label{7.9}
\Delta_{i}:=e_{i}^{T}\left(\frac{1}{n} X_{S}^{T} X_{S}\right)^{-1}\left[\frac{1}{n} X_{S}^{T} w-\lambda_{n} \operatorname{sgn}\left(\beta_{S}^{*}\right)\right],
\end{equation}
and note that, $\Delta_{i}=\check{\beta}_{S}-\beta_{S}^{*}$, when $\check{z}_{S}=\operatorname{sgn}\left(\beta_{S}^{*}\right)$.
Then the sign consistency condition is equivalent to  the assertion that for all $i \in S$, 
\begin{equation}\label{7.10}
\operatorname{sgn}\left(\beta_{i}^{*}+\Delta_{i}\right)=\operatorname{sgn}(\beta_{i}^{*})
\end{equation}
holds. To prove the strict dual feasibility condtion and sign consistency condition, now we impose the following assumptions:
\begin{assumption}\label{assumption 7.3}
	There exists some \textbf{incoherence parameter} $\gamma \in (0,1]$, such that
	\begin{equation}\label{7.11}
	\left\|X_{S^{c}}^{T} X_{S}\left(X_{S}^{T} X_{S}\right)^{-1}\right\|_{\infty} \leq(1-\gamma).
	\end{equation}
	And there exists some $C>0$ such that
	\begin{equation}\label{7.12}
	\lambda_{min}\left(\frac{1}{n} X_{S}^{T} X_{S}\right)\geq C_{min}.
	\end{equation}
\end{assumption}
\begin{theorem}
	Under Assumption \ref{assumption 7.3}, and the $n$-dimensional columns of the design matrix $X$ satisfy that $n^{-1 / 2} \max _{j \in S^{c}}\left\|X_{j}\right\| \leq 1$. Suppose that the sequence of regularization parameter $\{\lambda_n\}$ satisfies
	\begin{equation}\label{7.13}
	\lambda_{n}>\frac{2}{\gamma} \sqrt{\frac{2 \sigma^{2} \log p}{n}}.
	\end{equation}
	Then there exists some constant $c_1>0$, the following properties hold with probability greater than $1-2\exp \left(-c_{1} n \lambda_{n}^{2}\right) \rightarrow 1$.
	\begin{enumerate}
		\item The Lasso program has unqiue solution $\hat \beta$ and its support set contained within the ture support set, i.e., $S(\widehat{\beta}) \subseteq S\left(\beta^{*}\right)$. And $\hat \beta$ satisfies the $\ell_{\infty}$ bound
		\begin{equation}\label{7.14}
		\left\|\widehat{\beta}_{S}-\beta^{*}_{S}\right\|_{\infty} \leq \lambda_{n}\left[\left\|\left(X_{S}^{T} X_{S} / n\right)^{-1}\right\|_{\infty}+\frac{4 \sigma}{\sqrt{C_{\min }}}\right].
		\end{equation}
		\item If the minimum value of the ture parameter $\beta^{*}$ on its support is bounded below as $\beta_{min}>g(\lambda_n)$ where $g(\lambda_n)=\lambda_{n}\left[\left\|\left(X_{S}^{T} X_{S} / n\right)^{-1}\right\|_{\infty}+\frac{4 \sigma}{\sqrt{C_{\min }}}\right]$, then $\hat \beta$ has the correct signed support.
	\end{enumerate}
\end{theorem}
\begin{proof}
	Using the fact $|\check{z}_S|\leq 1$, (\ref{7.8}) and incoherence condition (\ref{7.11}), we have
	$$
	\check{z}_{S^c}\leq 1-\gamma+X_{S^c}^T\Pi_{X_{S}^{\perp}}\left(\frac{w}{\lambda_{n} n}\right).
	$$
	Let $\tilde{Z}_i=X_j^T\Pi_{X_{S}^{\perp}}\left(\frac{w}{\lambda_{n} n}\right),\ j\in S^c$, then 
	$$
	\max_{j \in S^{c}}	\check{z}_{j}\leq 1-\gamma+\max_{j \in S^{c}}\tilde{Z}_i.
	$$
	By the propertity of sub-gaussian random variable and the fact that the spectral norm of $\Pi_{X_{S}}$ is 1 and the condition $n^{-1 / 2} \max _{j \in S^{c}}\left\|X_{j}\right\| \leq 1$, the parameter of $\tilde{Z}_i$ is bounded by
	$$
	\frac{\sigma^{2}}{\lambda_{n}^{2} n^{2}}\left\|\Pi_{X_{S}^{\perp}}\left(X_{j}\right)\right\|_{2}^{2} \leq \frac{\sigma^{2}}{\lambda_{n}^{2} n}.
	$$
	Consequently, by tail probability bound of sub-gaussian variable and the uniform bound, we have
	$$
	\mathbb{P}\left[\max _{j \in S^{c}}\left|\widetilde{Z}_{j}\right| \geq t\right] \leq 2(p-k) \exp \left(-\frac{\lambda_{n}^{2} n t^{2}}{2 \sigma^{2}}\right).
	$$
	Set $t=\frac{\gamma}{2}$, we obtain
	\begin{align*}
	\mathbb{P}\left[\max _{j \in S^{c}}\left|\check{z}_{j}\right| > 1-\frac{\gamma}{2}\right]& \leq 2 \exp \left\{-\frac{\lambda_{n}^{2} n \gamma^{2}}{8 \sigma^{2}}+\log (p-k)\right\}\\
	&\leq 2 \exp \left\{-\log p+\log (p-k)\right\},
	\end{align*}
	then there exists some constant $c_1$ such that 
	$$
	\mathbb{P}\left[\max _{j \in S^{c}}\left|\check{z}_{j}\right| > 1-\frac{\gamma}{2}\right]\leq 2\exp \left(-c_{1} n \lambda_{n}^{2}\right).
	$$
	Thus with paobability greater than $1-2\exp \left(-c_{1} n \lambda_{n}^{2}\right)$, $S(\widehat{\beta}) \subseteq S\left(\beta^{*}\right)$ holds.
	By triangle inequality,
	$$
	\left\|\check{\beta}_{S}-\beta_{S}^{*}\right\|_{\infty}\leq \left\|\left(\frac{X_{S}^{T} X_{S}}{n}\right)^{-1} X_{S}^{T} \frac{w}{n}\right\|_{\infty}+\left\|\left(\frac{X_{S}^{T} X_{S}}{n}\right)^{-1}\right\|_{\infty} \lambda_{n}.
	$$
	For each $i\in S$, we define
	$$
	V_i:=e_i^T\left(\frac{X_{S}^{T} X_{S}}{n}\right)^{-1} X_{S}^{T} \frac{w}{n},
	$$
	obviously $V_i$ is a zero-mean sub-gaussian variable with parameter at most
	$$
	\frac{\sigma^{2}}{n}\left\|\left(\left(\frac{1}{n} X_{S}^{T} X_{S}\right)^{-1} \|\right)_{2} \leq \frac{\sigma^{2}}{C_{\min } n}\right.
	$$
	Then by sub-gaussian tail bound and the union bound we have
	$$
	\mathbb{P}\left[\max _{i=1, \ldots, k}\left|V_{i}\right|>t\right] \leq 2 \exp \left(-\frac{t^{2} C_{\min } n}{2 \sigma^{2}}+\log k\right),
	$$
	setting $t= 4 \sigma \lambda_{n} / \sqrt{C_{\min }}$ and using the fact $8 n \lambda_{n}^{2}>\log p \geq \log k$, we obtain
	$$
	\mathbb{P}\left[\max _{i=1, \ldots, k}\left|\bar{Z}_{i}\right|>4 \sigma \lambda_{n} / \sqrt{C_{\min }}\right]\leq 2\exp \left(-c_{1} n \lambda_{n}^{2}\right).
	$$
	Therefore, we have proved assertion 1 and assertion 2 follows immediately.
\end{proof}
\section{Some useful inequalities}
\begin{lemma}[Tail Bounds for $\chi^2$ Variables]
	Given a centralized $\chi^2$ variable $X$ with $d$ degrees of freedom, then for all $t \in (0,\frac{1}{2})$, we have
	\begin{align}
	\mathbb{P}[X \geq d(1+t)]& \leq \exp \left(-\frac{3}{16} d t^{2}\right),\\
	\mathbb{P}[X \leq(1-t) d]& \leq \exp \left(-\frac{1}{4} d t^{2}\right)
	\end{align}
\end{lemma}
\begin{lemma}[Concentration of spectral norms]
	For $k\leq n$, let $U \in \mathbb{R}^{n\times k}$ be a random matrix from the standard Gaussian random ensemble (i.e., $U_{i j} \sim N(0,1)$, i.i.d). Then for all $t>0$, we have
	\begin{equation}
	\mathbb{P}\left[\left\|\frac{1}{n} U^{T} U-I_{k \times k}\right\|_{2} \geq \delta(n, k, t)\right] \leq 2 \exp \left(-n t^{2} / 2\right),
	\end{equation}
	where $\delta(n, k, t):=2\left(\sqrt{\frac{k}{n}}+t\right)+\left(\sqrt{\frac{k}{n}}+t\right)^2$.
\end{lemma}
\begin{lemma}
	For $k\leq n$, let $X\in \mathbb{R}^{n\times k}$ have i.i.d rows $X_{i} \sim N(0, \Lambda)$.
	\begin{enumerate}
		\item If the covariance matrix $\Lambda$ has maximum eigenvalue $C_{max}<\infty$, then for all $t>0$, we have
		\begin{equation}
		\mathbb{P}\left[\left\|\frac{1}{n} X^{T} X-\Lambda\right\|_{2} \geq C_{\max } \delta(n, k, t)\right] \leq 2 \exp \left(-n t^{2} / 2\right).
		\end{equation}
		\item If the covariance matrix $\Lambda$ has minimum eigenvalue $C_{min}>0$, then for all $t>0$, we have
		\begin{equation}
		\mathbb{P}\left[\left\|\left(\frac{X^{T} X}{n}\right)^{-1}-\Lambda^{-1}\right\|_{2} \geq \frac{\delta(n, k, t)}{C_{\min }}\right] \leq 2 \exp \left(-n t^{2} / 2\right).
		\end{equation}
	\end{enumerate}
\end{lemma}
\bibliography{docbib}
\end{document}