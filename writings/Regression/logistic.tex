\section{High dimensional logistics regression}
The standard logistic regression reprensents the probability of Binomial distribution as 
\begin{equation}\label{3.1}
p(y=1 | \mathbf{x} ; \theta)=\sigma\left(\theta^{\top} \mathbf{x}\right)=\frac{1}{1+\exp \left(-\theta^{\top} \mathbf{x}\right)},
\end{equation}
where the regression parameter $\theta \in \mathbb{R}^p$ and predictor $\mathbf{x}\in \mathbb{R}^p$.
Given the data set $\{x_i, y_i, i=1,2,...,n\}$ and $y_i\in \{0, 1\}$, we can write the log-likelihood function as
\begin{equation}\label{3.2}
\ell\left(\beta_{0}, \beta\right)=\frac{1}{N} \sum_{i=1}^{N}\left[y_{i} \cdot\left(\beta_{0}+x_{i}^{T} \beta\right)-\log \left(1+e^{\beta_{0}+x_{i}^{T} \beta}\right)\right].
\end{equation}
And the MLE is $\hat{\beta}=\underset{\beta}{\arg \max } \ell\left(\beta_{0}, \beta\right)$, we can solve this by Newton algorithm. Taking the first order and second order derivative for $\ell\left(\beta_{0}, \beta\right) := \ell(\beta)$ on $\beta$ we have,
$$
\frac{\partial \ell(\beta)}{\partial \beta}=\sum_{i=1}^{n}\left(y_{i}-p_{i}\right) x_{i}, \quad \frac{\partial^{2} \ell(\beta)}{\partial \beta \partial \beta^{\prime}}=-\sum_{i=1}^{n} x_{i} x_{i}^{\prime} p_{i}\left(1-p_{i}\right)<0,
$$
which can be rewrite in matrix form
$$
\frac{\partial \ell(\beta)}{\partial \beta}=\mathbf{X}^T(\mathbf{y}-\mathbf{p}), \quad  \frac{\partial^{2} \ell(\beta)}{\partial \beta \partial \beta^{\prime}} = -\mathbf{X}^T\mathbf{WX},
$$
where $W=diag\left(p_1(1-p_1),...,p_n(1-p_n)\right)$.
Then we can obtain the Newton update step 
\begin{align*}
\beta^{\text {new }} &=\beta^{\text {old }}+\left(\mathbf{X}^{T} \mathbf{W} \mathbf{X}\right)^{-1} \mathbf{X}^{T}(\mathbf{y}-\mathbf{p}) \\
&=\left(\mathbf{X}^{T} \mathbf{W} \mathbf{X}\right)^{-1} \mathbf{X}^{T} \mathbf{W}\left(\mathbf{X} \beta^{\text {old }}+\mathbf{W}^{-1}(\mathbf{y}-\mathbf{p})\right) \\ 
&=\left(\mathbf{X}^{T} \mathbf{W} \mathbf{X}\right)^{-1} \mathbf{X}^{T} \mathbf{W} \mathbf{z}.
\end{align*}
Note that $\beta^{\text {new }}$ can be obtained by weighted LSE
\begin{equation}
\beta^{\text {new }} \leftarrow \arg \min _{\beta}(\mathbf{z}-\mathbf{X} \beta)^{T} \mathbf{W}(\mathbf{z}-\mathbf{X} \beta),
\end{equation}
therefore another name for the algorithm is \textit{Iteratively Reweighted Least Squares} (ISLR). Now considering the Newton Raphson algorithm:
\begin{equation}\label{3.4}
\widehat{\boldsymbol{\beta}}_{t+1} = \widehat{\boldsymbol{\beta}}_{t} - \frac{\partial^{2} \ell(\widehat{\boldsymbol{\beta}_t})}{\partial \beta \partial \beta^{\prime}}^{-1}\frac{\partial \ell(\widehat{\boldsymbol{\beta}_t})}{\partial \beta},
\end{equation}
and next we are going to bound the error after each iteration. 
\begin{theorem}\label{theorem3.1}
	Suppose that $X_i$ are sub-gaussian random vectors with mean $\boldsymbol{0}$, such that
	\begin{equation}\label{3.5}
	\left\|\mathbb{E}\left(X_iX_i^Tp_i\left(\boldsymbol{\beta}^{*}\right)\left(1-p_i\left(\boldsymbol{\beta}^{*}\right)\right)\right)\right\|_2 \geq \lambda_0>0
	\end{equation}
	and
	\begin{equation}\label{3.6}
	\|\Sigma\|_2=\|\mathbb{E}\left[X_iX_i^T\right]\|_2\leq \lambda_1
	\end{equation}
	Given initial estimator $\widehat{\boldsymbol{\beta}}_0$ satisfying  $\left\|\widehat{\boldsymbol{\beta}}_0-\boldsymbol{\beta}^{*}\right\|_2=O_p(a_n)$, then we have
	\begin{equation}
	\left\|\widehat{\boldsymbol{\beta}}_1-\boldsymbol{\beta}^{*}\right\|_2=O_p\left(a_n+\sqrt{\frac{p\log p}{n}}\right)
	\end{equation} 
\end{theorem}
\begin{proof}
	By (\ref{3.4}), 
	\begin{align*}
	\widehat{\boldsymbol{\beta}}_1-\boldsymbol{\beta}^{*}=\widehat{\boldsymbol{\beta}}_0-\boldsymbol{\beta}^{*}+\left(\frac{1}{n}\sum_{i=1}^{n} X_{i} X_{i}^{T} p_{i}(\widehat{\boldsymbol{\beta}}_0)\left(1-p_{i}(\widehat{\boldsymbol{\beta}}_0)\right)\right)^{-1}\left(\frac{1}{n}\sum_{i=1}^{n}\left(y_{i}-p_{i}(\widehat{\boldsymbol{\beta}}_0)\right) X_{i}\right),
	\end{align*}
	where $p_{i}\left(\widehat{\boldsymbol{\beta}}_0\right)=\exp(X_i^T\widehat{\boldsymbol{\beta}}_0)/\left(1+\exp(X_i^T\widehat{\boldsymbol{\beta}}_0)\right)$. Using assumption and it suffices to show that
	$$
	\sup_{\|\boldsymbol{\beta}-\boldsymbol{\beta}^{*}\|_2=O(a_n)}\left\|\left(\frac{1}{n}\sum_{i=1}^{n} X_{i} X_{i}^{T} p_{i}(\boldsymbol{\beta})\left(1-p_{i}(\boldsymbol{\beta})\right)\right)^{-1}\left(\frac{1}{n}\sum_{i=1}^{n}\left(y_{i}-p_{i}(\boldsymbol{\beta})\right) X_{i}\right)\right\|_2=O_p\left(a_n+\sqrt{\frac{p\log p}{n}}\right).
	$$
	Note that,
	 \begin{align*}
	 &\left\|\left(\frac{1}{n}\sum_{i=1}^{n} X_{i} X_{i}^{T} p_{i}(\boldsymbol{\beta})\left(1-p_{i}(\boldsymbol{\beta})\right)\right)^{-1}\left(\frac{1}{n}\sum_{i=1}^{n}\left(y_{i}-p_{i}(\boldsymbol{\beta})\right) X_{i}\right)\right\|_2 \\
	 \leq &\left\|\left(\frac{1}{n}\sum_{i=1}^{n} X_{i} X_{i}^{T} p_{i}(\boldsymbol{\beta})\left(1-p_{i}(\boldsymbol{\beta})\right)\right)^{-1}\right\|_2 \left\|\frac{1}{n}\sum_{i=1}^{n}\left(y_{i}-p_{i}(\boldsymbol{\beta})\right) X_{i}\right\|_2.
	 \end{align*}
	 To bound the inverse matrix operator norm, we make use of the following inequality: for any $p\times p$ matrix $A$
	 $$
	 \left\|(A+\Delta A)^{-1}-A^{-1}\right\|_{2} \leq\left\|A^{-1}\right\|_{2}^{2}\|\Delta A\|_{2}.
	 $$
	 First choose 
	 $$A= \frac{1}{n}\sum_{i=1}^{n} X_{i} X_{i}^{T} p_{i}(\boldsymbol{\beta}^{*})\left(1-p_{i}(\boldsymbol{\beta}^{*})\right)$$ 
	 and 
	 $$
	 \Delta A=\frac{1}{n}\sum_{i=1}^{n} X_{i} X_{i}^{T} \left[p_{i}(\boldsymbol{\beta})\left(1-p_{i}(\boldsymbol{\beta})\right)-p_{i}(\boldsymbol{\beta}^{*})\left(1-p_{i}(\boldsymbol{\beta}^{*})\right)\right].
	 $$
	 Then by definition of spectral norm, and there exists some $C>0$ such that
	 $$
	 \left|p_{i}(\boldsymbol{\beta})\left(1-p_{i}(\boldsymbol{\beta})\right)-p_{i}(\boldsymbol{\beta}^{*})\left(1-p_{i}(\boldsymbol{\beta}^{*})\right)\right|\leq C a_n,
	 $$
	 we obtain $\| \Delta A\|_2\leq Ca_n\left\|\frac{1}{n}\sum_{i=1}^{n} X_{i} X_{i}^{T}\right\|_2$. Then use concentration inequality of spectral norm, (\ref{3.5}) and (\ref{3.6}), we have
	 \begin{align*}
	 \sup_{\|\boldsymbol{\beta}-\boldsymbol{\beta}^{*}\|_2=O(a_n)}\left\|\left(\frac{1}{n}\sum_{i=1}^{n} X_{i} X_{i}^{T} p_{i}(\boldsymbol{\beta})\left(1-p_{i}(\boldsymbol{\beta})\right)\right)^{-1}\right\|_2 &\leq C\frac{a_n}{\lambda_0^2}\left\|\frac{1}{n}\sum_{i=1}^{n} X_{i} X_{i}^{T}\right\|_2 + \frac{1}{\lambda_0}.
	 \end{align*}
	 Then note that $y_i|X_i \sim b(1, p_{i}(\boldsymbol{\beta}^{*}))$, and let $\boldsymbol{G}_i=\left(y_{i}-p_{i}(\boldsymbol{\beta})\right) X_{i}$. 
	 Moreover,
	 $$
	 \left\|\mathbb{E}\left(\boldsymbol{G}_i\right)\right\|_2=\left\|\mathbb{E}\left[\left(p_i\left(\boldsymbol{\beta}^{*}\right)-p_i\left(\boldsymbol{\beta}\right)X_i\right)\right]\right\|_2\leq C\sqrt{p}a_n,
	 $$
	 and
	 $$
	\mathbb{E}\left(G_{ij}^2\exp(\eta|G_{ij}|)\right)\leq C^2a_n^2\mathbb{E}\left(X_{ij}^2\exp(\eta|X_{ij}|)\right)\leq C^2Ka_n^2
	 $$
	 then use the exponential inequality in \citet{cai2011constrained} again
	 \begin{align*}
	 \left\|\frac{1}{n}\sum_{i=1}^{n}\boldsymbol{G}_i\right\|_2&\leq \left\|\frac{1}{n}\sum_{i=1}^{n}\boldsymbol{G}_i-\mathbb{E}\left(\boldsymbol{G}_i\right)\right\|_2+\left\|\frac{1}{n}\sum_{i=1}^{n}\mathbb{E}\left(\boldsymbol{G}_i\right)\right\|_2\\
	 &=\left\|\frac{1}{n}\sum_{i=1}^{n}\boldsymbol{G}_i-\mathbb{E}\left(\boldsymbol{G}_i\right)\right\|_2+\left\|\mathbb{E}\left(\boldsymbol{G}_i\right)\right\|_2\\
	 &=O_p\left(a_n\sqrt{\frac{p\log p}{n}}\right)+O_p\left(\sqrt{p}a_n\right).
	 \end{align*}
\end{proof}

In the second iteration, initial estimator becomes $\widehat{\boldsymbol{\beta}}_1$, and initial error becomes $O_p\left(a_n+\sqrt{\frac{p\log p}{n}}\right)$. From the proof of Theorem \ref{theorem3.1}, we can obtain the $\ell_2$ error bound of the $t-$th step.
\begin{corollary}
	Under the condition of Theorem \ref{theorem3.1}, for the $t-$th iteration
	\begin{equation}
		\left\|\widehat{\boldsymbol{\beta}}_t-\boldsymbol{\beta}^{*}\right\|_2=O_p\left(a_n^t+\sqrt{\frac{p\log p}{n}}\right)
	\end{equation}
\end{corollary}
\textbf{Remark.} From the error bound of the $t-$th iteration, we can see that if initial estimator $\widehat{\boldsymbol{\beta}}_0$ isn't a consistent estimator, i.e., $a_n=O(1)$, then $\widehat{\boldsymbol{\beta}}_t$ isn't a consistent estimator either.